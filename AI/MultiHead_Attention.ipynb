{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        scale_factor = math.sqrt(self.head_dim)\n",
        "        scaled_attention_scores = attention_scores / scale_factor\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_scores = scaled_attention_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context_vector = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        Q = self.query_linear(query)\n",
        "        K = self.key_linear(key)\n",
        "        V = self.value_linear(value)\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        context_vector, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        context_vector = context_vector.transpose(1, 2).contiguous()\n",
        "        context_vector = context_vector.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.output_linear(context_vector)\n",
        "\n",
        "        return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 4\n",
        "    seq_len_q = 10\n",
        "    seq_len_kv = 12\n",
        "    d_model = 512\n",
        "    num_heads = 8\n",
        "    dropout = 0.1\n",
        "\n",
        "    query = torch.rand(batch_size, seq_len_q, d_model)\n",
        "    key = torch.rand(batch_size, seq_len_kv, d_model)\n",
        "    value = torch.rand(batch_size, seq_len_kv, d_model)\n",
        "\n",
        "    padding_mask = torch.ones(batch_size, 1, 1, seq_len_kv)\n",
        "    padding_mask[:, :, :, -2:] = 0\n",
        "\n",
        "    mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "    output = mha(query, key, value, mask=padding_mask)\n",
        "\n",
        "    print(\"Input Query Shape:\", query.shape)\n",
        "    print(\"Input Key Shape:\", key.shape)\n",
        "    print(\"Input Value Shape:\", value.shape)\n",
        "    print(\"Padding Mask Shape:\", padding_mask.shape)\n",
        "    print(\"Output Shape:\", output.shape)\n",
        "\n",
        "    self_attention_output = mha(query, query, query, mask=None)\n",
        "    print(\"Self-Attention Output Shape:\", self_attention_output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTV9L8yLdfhc",
        "outputId": "a43a45d8-b48a-4236-9000-13f32c13b4b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Query Shape: torch.Size([4, 10, 512])\n",
            "Input Key Shape: torch.Size([4, 12, 512])\n",
            "Input Value Shape: torch.Size([4, 12, 512])\n",
            "Padding Mask Shape: torch.Size([4, 1, 1, 12])\n",
            "Output Shape: torch.Size([4, 10, 512])\n",
            "Self-Attention Output Shape: torch.Size([4, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxU_A73jdfsX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
